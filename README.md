ğŸ§  Mercor AI Text Detection â€” DeBERTa & RoBERTa Ensemble

ğŸ† Top 100 Globally â€” Mercor AI Text Detection Challenge

ğŸ“˜ Overview

This project was developed for the Mercor AI Text Detection Challenge, a global competition focused on classifying whether a written response is AI-generated or human-written.
Out of thousands of participants worldwide, this solution ranked in the Top 100 globally, highlighting its strong performance and robust ensemble design.

We fine-tune two transformer-based models â€” DeBERTa-v3-small and RoBERTa-base â€” and combine their predictions using a weighted ensemble to achieve a balanced and accurate final classifier.

âš™ï¸ Environment

Frameworks & Libraries

ğŸ§© PyTorch 2.6.0 + cu124 â€” model training & GPU acceleration

ğŸ¤— Transformers 4.53.3 â€” pre-trained transformer architectures

ğŸ“¦ Datasets 4.4.1 â€” efficient text loading and tokenization

ğŸ§ª scikit-learn, pandas, numpy â€” preprocessing and evaluation

ğŸ“‚ Dataset

Path: /kaggle/input/mercor-ai-detection

File	Description
train.csv	Training data with is_cheating labels
test.csv	Unlabeled test data
sample_submission.csv	Submission template

Columns

Column	Description
id	Unique identifier
topic	Question or prompt
answer	Written response
is_cheating	Target (1 = AI-generated, 0 = human-written)
ğŸ§© Model Architecture
1. DeBERTa-v3-small

Disentangled attention, superior contextual understanding for nuanced text classification.

2. RoBERTa-base

Optimized BERT variant emphasizing pre-training robustness and text fluency.

3. Ensemble

Combines logits from both models:

ğ‘ƒ
ğ‘“
ğ‘–
ğ‘›
ğ‘
ğ‘™
=
ğ›¼
â‹…
ğ‘ƒ
ğ·
ğ‘’
ğµ
ğ¸
ğ‘…
ğ‘‡
ğ‘
+
(
1
âˆ’
ğ›¼
)
â‹…
ğ‘ƒ
ğ‘…
ğ‘œ
ğµ
ğ¸
ğ‘…
ğ‘‡
ğ‘
P
final
	â€‹

=Î±â‹…P
DeBERTa
	â€‹

+(1âˆ’Î±)â‹…P
RoBERTa
	â€‹


Weighted averaging yields smoother decision boundaries and improved F1 balance.

ğŸš€ Training Pipeline

Load and preprocess data (tokenization, truncation).

Fine-tune both transformer models on is_cheating.

Evaluate using F1 Score.

Save model checkpoints and inference outputs.

Blend predictions via ensemble weighting for submission.

ğŸ“ˆ Evaluation Metric

F1 Score â€” harmonic mean of precision and recall:

ğ¹
1
=
2
Ã—
(
ğ‘
ğ‘Ÿ
ğ‘’
ğ‘
ğ‘–
ğ‘ 
ğ‘–
ğ‘œ
ğ‘›
Ã—
ğ‘Ÿ
ğ‘’
ğ‘
ğ‘
ğ‘™
ğ‘™
)
ğ‘
ğ‘Ÿ
ğ‘’
ğ‘
ğ‘–
ğ‘ 
ğ‘–
ğ‘œ
ğ‘›
+
ğ‘Ÿ
ğ‘’
ğ‘
ğ‘
ğ‘™
ğ‘™
F1=
precision+recall
2Ã—(precisionÃ—recall)
	â€‹


Chosen to reward balanced accuracy across both AI-generated and human-written classes.
