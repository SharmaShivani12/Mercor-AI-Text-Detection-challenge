ğŸ§  Mercor AI Text Detection â€” DeBERTa & RoBERTa Ensemble

ğŸ† Top 100 Globally â€” Mercor AI Text Detection Challenge

ğŸ“˜ Overview

This project was developed for the Mercor AI Text Detection Challenge, a global competition focused on classifying whether a written response is AI-generated or human-written.
Out of thousands of participants worldwide, this solution ranked in the Top 100 globally, highlighting its strong performance and robust ensemble design.

We fine-tune two transformer-based models â€” DeBERTa-v3-small and RoBERTa-base â€” and combine their predictions using a weighted ensemble to achieve a balanced and accurate final classifier.

âš™ï¸ Environment

Frameworks & Libraries

ğŸ§© PyTorch 2.6.0 + cu124 â€” model training & GPU acceleration

ğŸ¤— Transformers 4.53.3 â€” pre-trained transformer architectures

ğŸ“¦ Datasets 4.4.1 â€” efficient text loading and tokenization

ğŸ§ª scikit-learn, pandas, numpy â€” preprocessing and evaluation

ğŸ“‚ Dataset

Path: /kaggle/input/mercor-ai-detection

File	Description
train.csv	Training data with is_cheating labels
test.csv	Unlabeled test data
sample_submission.csv	Submission template

Columns

Column	Description
id	Unique identifier
topic	Question or prompt
answer	Written response
is_cheating	Target (1 = AI-generated, 0 = human-written)
ğŸ§© Model Architecture
1. DeBERTa-v3-small

Disentangled attention, superior contextual understanding for nuanced text classification.

2. RoBERTa-base

Optimized BERT variant emphasizing pre-training robustness and text fluency.

3. Ensemble

ğŸ¤ Why Use an Ensemble?Â¶
An ensemble combines predictions from multiple models to make a single, more stable prediction.
Instead of relying on one modelâ€™s biases or weaknesses, the ensemble leverages their strengths â€” like averaging multiple expert opinions.

In this notebook: ```python final_preds = (deberta_preds + roberta_preds) / 2
	â€‹
Weighted averaging yields smoother decision boundaries and improved F1 balance.

ğŸš€ Training Pipeline

Load and preprocess data (tokenization, truncation).

Fine-tune both transformer models on is_cheating.

Evaluate using F1 Score.

Save model checkpoints and inference outputs.

Blend predictions via ensemble weighting for submission.

ğŸ“ˆ Evaluation Metric

F1 Score â€” harmonic mean of precision and recall

F1=
precision+recall
2Ã—(precisionÃ—recall)
	â€‹


Chosen to reward balanced accuracy across both AI-generated and human-written classes.
